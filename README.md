# üöÄ Cloud NLP Text Classification on GCP

An end-to-end text classification pipeline for hate speech detection, demonstrating cloud computing, deep neural networks, and performance analysis.

## üìã Project Overview

This project implements a production-grade hate speech classification system with:

- **Classical ML Baselines**: TF-IDF + Logistic Regression/SVM
- **Transformer Model**: Fine-tuned DistilBERT with advanced training optimizations
- **REST API**: FastAPI server with comprehensive error handling and validation
- **Containerization**: Docker with multi-environment support (dev/prod)
- **Cloud Training**: GCP GPU VM support for efficient model training
- **Cloud Deployment**: Google Cloud Platform (Cloud Run) - Ready for deployment
- **Performance Analysis**: Offline metrics, online latency/throughput, cost analysis
- **Testing Suite**: Comprehensive unit and integration tests

### üéØ Project Status

| Phase | Status | Description |
|-------|--------|-------------|
| **Phase 1**: Data Pipeline | ‚úÖ Complete | Dataset download, preprocessing, train/val/test splits |
| **Phase 2**: Baseline Models | ‚úÖ Complete | TF-IDF + LogReg/SVM with full evaluation |
| **Phase 3**: Transformer Training | ‚úÖ Complete | DistilBERT fine-tuning with advanced features |
| **Phase 4**: FastAPI Server | ‚úÖ Complete | Production-ready API with Pydantic V2 |
| **Phase 5**: Dockerization | ‚úÖ Complete | Multi-environment Docker setup |
| **Phase 6**: Cloud Deployment | üöÄ Ready | Prepared for GCP Cloud Run deployment |

**Progress**: 5/6 phases complete (83%)

## üèóÔ∏è Architecture

```
Data Pipeline ‚Üí Model Training ‚Üí Evaluation ‚Üí API Deployment ‚Üí Cloud
     ‚Üì              ‚Üì               ‚Üì            ‚Üì              ‚Üì
  Raw CSV    Baselines +      Accuracy      FastAPI      GCP Cloud Run
             Transformer      F1 Score      + Docker
```

## üìÅ Project Structure

```
cloud-nlp-classification-gcp/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ data/              # Data processing modules
‚îÇ   ‚îú‚îÄ‚îÄ models/            # Model training & evaluation
‚îÇ   ‚îî‚îÄ‚îÄ api/               # FastAPI server
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/               # Original datasets
‚îÇ   ‚îî‚îÄ‚îÄ processed/         # Train/val/test splits
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ baselines/         # Saved baseline models
‚îÇ   ‚îî‚îÄ‚îÄ transformer/       # Saved DistilBERT model
‚îú‚îÄ‚îÄ config/                # YAML configuration files
‚îú‚îÄ‚îÄ scripts/               # Executable scripts
‚îú‚îÄ‚îÄ notebooks/             # Jupyter notebooks (EDA)
‚îú‚îÄ‚îÄ tests/                 # Unit & integration tests
‚îú‚îÄ‚îÄ Dockerfile             # Container definition
‚îî‚îÄ‚îÄ requirements.txt       # Python dependencies
```

## üöÄ Quick Start

### 1. Environment Setup

```bash
# Create virtual environment
python -m venv venv

# Activate virtual environment
# On Windows:
venv\Scripts\activate
# On Linux/Mac:
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Download Dataset

```bash
# Download hate speech dataset from Hugging Face
python scripts/download_dataset.py
```

This will download and prepare the hate speech dataset at `data/raw/dataset.csv`.

### 3. Preprocess Data

```bash
# Cross-platform Python script (Recommended)
python run_preprocess.py

# Or run directly:
python -m src.data.preprocess

# Or use platform-specific scripts:
# Windows: .\scripts\run_preprocess_local.ps1
# Linux/Mac: bash scripts/run_preprocess_local.sh
```

This creates train/val/test splits in `data/processed/`.

### 4. Train Baseline Models

```bash
# Cross-platform Python script (Recommended)
python run_baselines.py

# Or run directly:
python -m src.models.train_baselines

# Or use platform-specific scripts:
# Windows: .\scripts\run_baselines_local.ps1
# Linux/Mac: bash scripts/run_baselines_local.sh
```

### 5. Train Transformer Model

#### Local Training (Quick Testing - 3 epochs)

```bash
# Cross-platform Python script (Recommended)
python run_transformer.py

# Or run directly:
python -m src.models.transformer_training

# Or use platform-specific scripts:
# Windows: .\scripts\run_transformer_local.ps1
# Linux/Mac: bash scripts/run_transformer_local.sh
```

**Local Training Specs:**
- Epochs: 3 (quick testing)
- Batch Size: 32
- Max Sequence Length: 128
- Expected Accuracy: 85-88%
- Training Time: 1-2 hours (CPU), 15-25 min (GPU)

**Note:** Transformer training requires a GPU for reasonable speed. Set `device: "cpu"` in `config/config_transformer.yaml` if GPU is unavailable.

#### Cloud Training (Production - 10 epochs)

For production-quality models with optimized settings:

```bash
# Using cloud configuration locally (if you have a GPU)
python -m src.models.transformer_training \
  --config config/config_transformer_cloud.yaml \
  --mode cloud \
  --fp16

# Or use the cloud training script:
# Windows: .\scripts\run_transformer_cloud.ps1
# Linux/Mac: bash scripts/run_gcp_training.sh
```

**Cloud Training Specs:**
- Epochs: 10 (production quality)
- Batch Size: 64
- Max Sequence Length: 256
- FP16: Enabled (mixed precision)
- LR Scheduler: Cosine with warmup
- Expected Accuracy: 90-93%
- Training Time: 20-40 min (T4 GPU), 15-25 min (V100)

#### Advanced Training Options

```bash
# Override specific parameters
python -m src.models.transformer_training \
  --config config/config_transformer.yaml \
  --mode local \
  --epochs 5 \
  --batch-size 32 \
  --learning-rate 3e-5 \
  --fp16 \
  --output-dir models/transformer/custom

# Disable early stopping
python -m src.models.transformer_training \
  --no-early-stopping

# Use different learning rate scheduler
# Edit config YAML: lr_scheduler.type = "cosine" | "linear" | "polynomial"
```

**Advanced Training Features:**
- ‚úÖ **Early Stopping**: Configurable patience based on validation F1/loss
- ‚úÖ **LR Schedulers**: Linear, cosine, cosine_with_restarts, polynomial, constant, constant_with_warmup
- ‚úÖ **Mixed Precision (FP16)**: Faster GPU training with automatic fallback
- ‚úÖ **Gradient Accumulation**: Effective larger batch sizes on limited memory
- ‚úÖ **Warmup Steps**: Configurable warmup ratio or fixed steps
- ‚úÖ **DataLoader Optimizations**: Multi-worker loading with pin_memory
- ‚úÖ **CLI Overrides**: Override any config parameter via command line
- ‚úÖ **Automatic Best Model Selection**: Saves best checkpoint based on validation metrics
- ‚úÖ **Comprehensive Logging**: Training progress, metrics, and timing information

### 6. Run API Server (Local)

```bash
# Start FastAPI server
uvicorn src.api.server:app --host 0.0.0.0 --port 8000 --reload

# Or use the shell script:
bash scripts/run_api_local.sh
```

Test the API:

```bash
# Health check
curl http://localhost:8000/health

# Prediction
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d '{"text": "This is a test message"}'
```

### 7. Run Tests

```bash
# Run all tests with the universal test runner
python run_tests.py

# Or use pytest directly
pytest tests/ -v

# Run specific test categories
pytest tests/test_basic_imports.py      # Import tests
pytest tests/test_api.py                # API tests
pytest tests/test_baseline_inference.py # Baseline model tests
pytest tests/test_advanced_training.py  # Training configuration tests

# Run with coverage
pytest tests/ --cov=src --cov-report=html
```

**Test Suite Coverage:**
- ‚úÖ Import validation (6 tests)
- ‚úÖ API endpoints (health, predict, docs)
- ‚úÖ Model loading and inference
- ‚úÖ Configuration validation
- ‚úÖ Error handling
- ‚úÖ Pydantic V2 compliance (zero deprecation warnings)

## üê≥ Docker Deployment

### Multi-Model Support üéØ

The Docker image now includes **3 models** with dynamic switching:

| Model | Type | Speed | Accuracy | Use Case |
|-------|------|-------|----------|----------|
| **distilbert** | Transformer | ~50ms | 90-93% | Best accuracy |
| **logistic_regression** | Baseline | ~5ms | 85-88% | Fast inference |
| **linear_svm** | Baseline | ~5ms | 85-88% | Robust predictions |

**Key Features:**
- ‚úÖ All models included in single Docker image
- ‚úÖ Switch models via API without restarting container
- ‚úÖ Choose default model via environment variable
- ‚úÖ Production-ready with health checks and monitoring

### Prerequisites

Before building the Docker image, ensure you have:

1. **Docker installed**: [Install Docker](https://docs.docker.com/get-docker/)
2. **Trained models**: 
   - Transformer: `models/transformer/distilbert/`
   - Baselines: `models/baselines/*.joblib`
3. **Sufficient disk space**: ~2-3 GB for the Docker image

### Build Docker Image

```bash
# Build the image (this may take 5-10 minutes)
docker build -t cloud-nlp-classifier .

# Optional: Build with a specific tag
docker build -t cloud-nlp-classifier:v1.0 .

# Optional: Build with no cache (for clean rebuild)
docker build --no-cache -t cloud-nlp-classifier .
```

**Build Details:**
- Base image: `python:3.11-slim`
- Image size: ~2 GB (includes PyTorch, transformers, and model weights)
- Build time: 5-10 minutes (depending on network speed)

### Run Docker Container

**Option 1: Using Docker Compose (Recommended)**

```bash
# Start services
docker-compose up -d

# View logs
docker-compose logs -f

# Stop services
docker-compose down
```

**Option 2: Using Docker CLI**

```bash
# Run with default model (DistilBERT - best accuracy)
docker run -d -p 8000:8000 --name nlp-api cloud-nlp-classifier

# Run with fast model (Logistic Regression - 10x faster)
docker run -d -p 8000:8000 -e DEFAULT_MODEL=logistic_regression --name nlp-api cloud-nlp-classifier

# Run with Linear SVM model
docker run -d -p 8000:8000 -e DEFAULT_MODEL=linear_svm --name nlp-api cloud-nlp-classifier

# Run with custom port mapping
docker run -d -p 9000:8000 --name nlp-api cloud-nlp-classifier

# Run with debug logging
docker run -d -p 8000:8000 -e LOG_LEVEL=debug --name nlp-api cloud-nlp-classifier
```

**Environment Variables:**
- `DEFAULT_MODEL`: Choose startup model (`distilbert`, `logistic_regression`, `linear_svm`)
- `LOG_LEVEL`: Logging level (`INFO`, `DEBUG`, `WARNING`, `ERROR`)
```

**Docker Compose Configurations:**
- `docker-compose.yml` - Standard deployment with health checks and resource limits
- `docker-compose.dev.yml` - Development with hot-reload, debug logging, and volume mounts
- `docker-compose.prod.yml` - Production with 4 workers, optimized resources, and restart policies

**Docker Compose Usage:**
```bash
# Standard deployment
docker-compose up -d

# Development mode (with hot-reload)
docker-compose -f docker-compose.dev.yml up

# Production mode (multiple workers)
docker-compose -f docker-compose.prod.yml up -d

# View logs
docker-compose logs -f api

# Stop services
docker-compose down
```

### Test the Containerized API

```bash
# Health check (shows current model and available models)
curl http://localhost:8000/health

# List all available models
curl http://localhost:8000/models

# Make a prediction with current model
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d '{"text": "This is a test message"}'

# Switch to a different model (without restarting container!)
curl -X POST http://localhost:8000/models/switch \
  -H "Content-Type: application/json" \
  -d '{"model_name": "logistic_regression"}'

# Make prediction with new model
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d '{"text": "This is a test message"}'

# Access interactive API documentation
# Open in browser: http://localhost:8000/docs
```

### Docker Management Commands

```bash
# View running containers
docker ps

# View all containers (including stopped)
docker ps -a

# View container logs
docker logs nlp-api

# Follow logs in real-time
docker logs -f nlp-api

# Stop the container
docker stop nlp-api

# Start a stopped container
docker start nlp-api

# Remove the container
docker rm nlp-api

# Remove the image
docker rmi cloud-nlp-classifier

# View image details
docker inspect cloud-nlp-classifier

# Check image size
docker images cloud-nlp-classifier
```

### Docker Health Check

The container includes a built-in health check that runs every 30 seconds:

```bash
# Check container health status
docker inspect --format='{{.State.Health.Status}}' nlp-api

# View health check logs
docker inspect --format='{{json .State.Health}}' nlp-api | python -m json.tool
```

Health states:
- `starting`: Container is starting up (first 40 seconds grace period)
- `healthy`: API is responding correctly to /health endpoint
- `unhealthy`: API failed 3 consecutive health checks

**Health Check Configuration:**
- Interval: 30 seconds
- Timeout: 10 seconds
- Retries: 3
- Start Period: 40 seconds (allows model loading time)

### Troubleshooting

**Container won't start:**
```bash
# Check logs for errors
docker logs nlp-api

# Run interactively to see startup issues
docker run -it -p 8000:8000 cloud-nlp-classifier
```

**Port already in use:**
```bash
# Use a different port
docker run -p 8001:8000 cloud-nlp-classifier
```

**Out of memory:**
```bash
# Increase Docker memory limit in Docker Desktop settings
# Or run with memory limit
docker run -p 8000:8000 --memory="2g" cloud-nlp-classifier
```

**Model not found error:**
```bash
# Ensure the model is trained before building the image
python run_transformer.py

# Verify model files exist
ls -la models/transformer/distilbert/
```

## ‚òÅÔ∏è GCP Cloud Training

### Why Train on GCP?

Training transformer models on cloud GPUs offers significant advantages:
- **Speed**: 10-50x faster than CPU training
- **Scalability**: Access to powerful GPUs (T4, V100, A100)
- **Cost-Effective**: Pay only for training time (~$0.35-$2.50/hour)
- **No Local Setup**: No need for expensive local GPU hardware

### GCP GPU VM Setup

#### 1. Create a GCP GPU VM Instance

```bash
# Set your GCP project
export PROJECT_ID=your-gcp-project-id
gcloud config set project $PROJECT_ID

# Enable Compute Engine API
gcloud services enable compute.googleapis.com

# Create a GPU VM instance (NVIDIA T4 GPU)
gcloud compute instances create nlp-training-vm \
  --zone=us-central1-a \
  --machine-type=n1-standard-4 \
  --accelerator=type=nvidia-tesla-t4,count=1 \
  --image-family=pytorch-latest-gpu \
  --image-project=deeplearning-platform-release \
  --boot-disk-size=100GB \
  --maintenance-policy=TERMINATE \
  --metadata="install-nvidia-driver=True"

# Alternative: Use V100 for faster training (more expensive)
# --machine-type=n1-standard-8 \
# --accelerator=type=nvidia-tesla-v100,count=1
```

**GPU Options & Pricing (us-central1):**
- **T4** (16GB): ~$0.35/hour - Good for DistilBERT
- **V100** (16GB): ~$2.48/hour - Faster training
- **A100** (40GB): ~$3.67/hour - Largest models

#### 2. SSH into the VM

```bash
# SSH into the instance
gcloud compute ssh nlp-training-vm --zone=us-central1-a

# Verify GPU is available
nvidia-smi
```

#### 3. Setup Training Environment

```bash
# Clone your repository
git clone https://github.com/YOUR_USERNAME/CLOUD-NLP-CLASSIFIER-GCP.git
cd CLOUD-NLP-CLASSIFIER-GCP

# Run the setup script (installs dependencies, downloads data)
bash scripts/setup_gcp_training.sh

# This script will:
# - Install Python 3.11 and dependencies
# - Install CUDA drivers (if needed)
# - Create virtual environment
# - Install PyTorch with GPU support
# - Download and preprocess dataset
```

#### 4. Start Training

```bash
# Activate virtual environment
source venv/bin/activate

# Start training with cloud configuration
bash scripts/run_gcp_training.sh

# Or run directly with custom settings
python -m src.models.transformer_training \
  --config config/config_transformer_cloud.yaml \
  --mode cloud \
  --fp16 \
  --epochs 10 \
  --batch-size 64
```

**Training will:**
- Use optimized cloud settings (FP16, larger batches)
- Save model to `models/transformer/distilbert_cloud/`
- Log training progress to `training.log`
- Take approximately 20-40 minutes on T4 GPU

#### 5. Monitor Training

```bash
# In another terminal, SSH and monitor
gcloud compute ssh nlp-training-vm --zone=us-central1-a

# Watch GPU usage
watch -n 1 nvidia-smi

# Follow training logs
tail -f models/transformer/distilbert_cloud/training.log
```

#### 6. Download Trained Model

```bash
# From your local machine, download the trained model
gcloud compute scp --recurse \
  nlp-training-vm:~/CLOUD-NLP-CLASSIFIER-GCP/models/transformer/distilbert_cloud \
  ./models/transformer/ \
  --zone=us-central1-a

# Or use Google Cloud Storage (recommended for large models)
# On VM: Upload to GCS
gsutil -m cp -r models/transformer/distilbert_cloud gs://your-bucket-name/models/

# On local: Download from GCS
gsutil -m cp -r gs://your-bucket-name/models/distilbert_cloud ./models/transformer/
```

#### 7. Stop/Delete VM (Important!)

```bash
# Stop the VM (can restart later, still charges for disk)
gcloud compute instances stop nlp-training-vm --zone=us-central1-a

# Delete the VM (no charges, but need to recreate)
gcloud compute instances delete nlp-training-vm --zone=us-central1-a
```

**üí° Cost Saving Tips:**
- Delete VM immediately after training completes
- Use preemptible instances for 60-80% cost savings (may be interrupted)
- Use Cloud Storage for model artifacts instead of large boot disks
- Monitor training with early stopping to avoid unnecessary epochs
- Use T4 GPUs for DistilBERT (V100/A100 overkill for this model size)
- Set up billing alerts to avoid unexpected charges

**Estimated Training Costs (10 epochs):**
- T4 GPU (30-40 min): ~$0.20-$0.25
- V100 GPU (15-25 min): ~$0.60-$1.00
- A100 GPU (10-15 min): ~$0.60-$0.90

### Local vs Cloud Configuration Comparison

| Setting | Local (Testing) | Cloud (Production) |
|---------|----------------|-------------------|
| **Batch Size** | 32 | 64 |
| **Epochs** | 3 | 10 |
| **Max Seq Length** | 128 | 256 |
| **FP16** | Optional | Enabled |
| **Gradient Accumulation** | 1 | 2 |
| **LR Scheduler** | Linear | Cosine |
| **Early Stopping** | 3 patience | 5 patience |
| **Training Time** | 1-2 hours (CPU) | 20-40 min (T4 GPU) |
| **Expected Accuracy** | 85-88% | 90-93% |
| **Expected F1 Score** | 0.82-0.85 | 0.88-0.91 |

## ‚òÅÔ∏è GCP Deployment (Inference)

### Prerequisites

1. GCP project with billing enabled
2. `gcloud` CLI installed and authenticated
3. Enable required APIs:
   ```bash
   gcloud services enable run.googleapis.com
   gcloud services enable artifactregistry.googleapis.com
   ```

### Deploy to Cloud Run

```bash
# Set project ID
export PROJECT_ID=your-gcp-project-id

# Build and push to Artifact Registry
gcloud builds submit --tag gcr.io/$PROJECT_ID/cloud-nlp-classifier

# Deploy to Cloud Run
gcloud run deploy cloud-nlp-classifier \
  --image gcr.io/$PROJECT_ID/cloud-nlp-classifier \
  --platform managed \
  --region us-central1 \
  --allow-unauthenticated \
  --memory 2Gi \
  --cpu 2
```

## üìä Performance Analysis

### Offline Evaluation

Models are evaluated on:
- **Accuracy**: Overall correctness
- **F1 Score**: Macro and weighted F1
- **Precision/Recall**: Per-class performance
- **ROC-AUC**: For binary classification

### Online Evaluation

API performance metrics:
- **Latency**: p50, p95, p99 response times
- **Throughput**: Requests per second
- **Cold Start**: Initial request latency

### Cost Analysis

- **Training Cost**: GPU VM hours
- **Inference Cost**: Cloud Run pricing per 1K requests
- **Storage Cost**: Model artifacts in Artifact Registry

## üîß Configuration

### Baseline Models (`config/config_baselines.yaml`)

```yaml
vectorizer:
  type: "tfidf"
  max_features: 10000
  ngram_range: [1, 2]

logistic_regression:
  C: 1.0
  max_iter: 1000
  class_weight: "balanced"
```

### Transformer Model (`config/config_transformer.yaml`)

```yaml
model:
  name: "distilbert-base-uncased"
  max_seq_length: 128

training:
  train_batch_size: 16
  learning_rate: 2.0e-5
  num_train_epochs: 3
  early_stopping:
    enabled: true
    patience: 3
```

## üìà Expected Results

### Baseline Models (TF-IDF + LogReg/SVM)
- **Accuracy**: 75-85%
- **F1 Score**: 0.70-0.80 (macro)
- **Training Time**: <5 minutes
- **Inference Latency**: <10ms per sample
- **Model Size**: <50 MB

### Transformer Model (DistilBERT)

**Local Training (3 epochs):**
- **Accuracy**: 85-88%
- **F1 Score**: 0.82-0.85 (macro)
- **Training Time**: 1-2 hours (CPU), 15-25 min (GPU)
- **Inference Latency**: 45-60ms (p50), 80-100ms (p95)
- **Model Size**: ~250 MB

**Cloud Training (10 epochs):**
- **Accuracy**: 90-93%
- **F1 Score**: 0.88-0.91 (macro)
- **Training Time**: 20-40 min (T4 GPU)
- **Inference Latency**: 45-60ms (p50), 80-100ms (p95)
- **Model Size**: ~250 MB

### API Performance
- **Startup Time**: 5-8 seconds (model loading)
- **Memory Usage**: ~1.2 GB (active inference)
- **Throughput**: 20-50 requests/second (single worker)
- **Cold Start**: <10 seconds (Docker container)

## üß™ Development Workflow

1. **Data Exploration**: Use `notebooks/eda.ipynb` for EDA (optional)
2. **Experiment**: Modify configs in `config/` or use CLI overrides
3. **Train Locally**: Quick iteration with 3 epochs
4. **Train on Cloud**: Production training with 10 epochs on GCP GPU VM
5. **Evaluate**: Check metrics, logs, and model performance
6. **Test API**: Run locally with `uvicorn` or Docker
7. **Run Tests**: Validate with comprehensive test suite
8. **Deploy**: Docker ‚Üí GCP Cloud Run
9. **Monitor**: Track performance, latency, and costs

## üõ†Ô∏è Development Tools

### Cross-Platform Scripts
All major operations have universal Python scripts that work on Windows, Linux, and Mac:
- `run_preprocess.py` - Data preprocessing
- `run_baselines.py` - Baseline model training
- `run_transformer.py` - Transformer training
- `run_tests.py` - Test suite execution

### Platform-Specific Scripts
Alternative scripts for shell/PowerShell users:
- `scripts/*.sh` - Bash scripts (Linux/Mac)
- `scripts/*.ps1` - PowerShell scripts (Windows)

### Configuration Files
- `config/config_baselines.yaml` - Baseline model hyperparameters
- `config/config_transformer.yaml` - Local training config (3 epochs)
- `config/config_transformer_cloud.yaml` - Cloud training config (10 epochs)

## üìù Course Relevance

### Cloud Computing ‚òÅÔ∏è
- GCP services (Cloud Run, Artifact Registry)
- Containerization and orchestration
- Auto-scaling and load balancing

### Deep Neural Networks üß†
- Transformer architecture (DistilBERT)
- Transfer learning and fine-tuning
- Optimization techniques (early stopping, LR scheduling)

### Performance Analysis üìä
- Offline metrics (accuracy, F1, ROC-AUC)
- Online metrics (latency, throughput)
- Cost-benefit analysis

## ü§ù Contributing

This is a course project. For team collaboration:

1. Create feature branches
2. Make changes and test locally
3. Submit pull requests for review
4. Merge after approval

## üìÑ License

This project is for educational purposes.

## üôè Acknowledgments

- **Dataset**: Hate Speech Offensive dataset from Hugging Face (24,783 samples)
- **Model**: DistilBERT from Hugging Face Transformers
- **Frameworks**: FastAPI, PyTorch, Transformers, scikit-learn
- **Infrastructure**: Docker, GCP (Cloud Run, Compute Engine)
- **Testing**: pytest, Pydantic V2

## üìö Additional Documentation

Comprehensive guides available in the `docs/` directory:
- `MULTI_MODEL_DOCKER_GUIDE.md` - **NEW!** Multi-model Docker deployment (500+ lines)
- `DOCKER_GUIDE.md` - Complete Docker documentation (650+ lines)
- `DOCKER_COMPOSE_GUIDE.md` - Docker Compose best practices (600+ lines)
- `PHASE10_ADVANCED_TRAINING_SUMMARY.md` - Advanced training features
- `CROSS_PLATFORM_GUIDE.md` - Cross-platform development guide
- `PROJECT_STATUS.md` - Current project status and metrics

## üìû Contact

For questions or issues, please open an issue in the repository.

---

**Built with ‚ù§Ô∏è for cloud-based NLP classification**
