# Backend vs Frontend Deployment Comparison

## ğŸ“Š Side-by-Side Analysis

This document compares the backend API deployment (WORKING âœ…) with the planned frontend UI deployment to ensure we follow the same proven pattern.

---

## ğŸ—ï¸ Deployment Pattern Comparison

| Aspect | Backend API (DONE âœ…) | Frontend UI (PLANNED) |
|--------|----------------------|----------------------|
| **VM** | nlp-classifier-vm | Same VM (nlp-classifier-vm) |
| **Container Name** | `nlp-api` | `nlp-ui` |
| **Port** | 8000 | 8501 |
| **Dockerfile** | `Dockerfile` | `Dockerfile.streamlit.api` (new) |
| **Image Name** | `cloud-nlp-classifier:latest` | `cloud-nlp-ui:latest` |
| **Models** | Included in image (~2.5GB) | NOT included (API mode) |
| **Size** | ~2.5GB | ~500MB (much smaller) |
| **Dependencies** | PyTorch, transformers, FastAPI | Streamlit, requests (lightweight) |
| **Health Check** | `/health` | `/_stcore/health` |
| **External URL** | http://35.232.76.140:8000 | http://35.232.76.140:8501 |

---

## ğŸ“‹ Deployment Script Comparison

### Backend Script Pattern (gcp-complete-deployment.ps1)

```powershell
# PHASE 1: Upload Models to GCS
âœ… Create/verify GCS bucket
âœ… Upload models (770 MB with -NoCheckpoints)
âœ… Verify uploads

# PHASE 2: Verify/Start VM
âœ… Check VM status
âœ… Start VM if stopped

# PHASE 3: Deploy Application
âœ… Clone repository from GitHub
âœ… Download models from GCS
âœ… Build Docker image (5-10 min)
âœ… Stop old container
âœ… Run new container
âœ… Test health (internal)
âœ… Test health (external)

# PHASE 4: Summary
âœ… Show endpoints
âœ… Show duration
âœ… Show costs
```

### Frontend Script Pattern (gcp-deploy-ui.ps1) - TO CREATE

```powershell
# PHASE 1: Verify API is Running
âœ… Check nlp-api container status
âœ… Test API health endpoint
âœ… Verify API is accessible

# PHASE 2: Verify/Update Firewall
âœ… Check port 8501 firewall rule
âœ… Create rule if missing

# PHASE 3: Deploy UI Application
âœ… SSH into VM (same as backend)
âœ… cd ~/CLOUD-NLP-CLASSIFIER-GCP (same directory)
âœ… Pull latest code (git pull)
âœ… Build UI Docker image (2-3 min, much faster)
âœ… Stop old nlp-ui container
âœ… Run new UI container with API_URL
âœ… Test health (internal)
âœ… Test health (external)
âœ… Test API connectivity from UI

# PHASE 4: Summary
âœ… Show UI URL
âœ… Show API URL
âœ… Show duration
âœ… No cost increase
```

---

## ğŸ”§ Docker Commands Comparison

### Backend Container

```bash
# Build
sudo docker build -t cloud-nlp-classifier:latest .

# Run
sudo docker run -d \
    --name nlp-api \
    -p 8000:8000 \
    --restart unless-stopped \
    cloud-nlp-classifier:latest

# Health Check
curl http://localhost:8000/health

# Logs
sudo docker logs -f nlp-api
```

### Frontend Container (PLANNED)

```bash
# Build
sudo docker build -f Dockerfile.streamlit.api -t cloud-nlp-ui:latest .

# Run
sudo docker run -d \
    --name nlp-ui \
    -p 8501:8501 \
    -e API_URL=http://localhost:8000 \
    --restart unless-stopped \
    cloud-nlp-ui:latest

# Health Check
curl http://localhost:8501/_stcore/health

# Logs
sudo docker logs -f nlp-ui
```

---

## ğŸŒ Network Architecture

### Current (API Only)
```
Internet
   â”‚
   â–¼
Firewall (port 8000)
   â”‚
   â–¼
VM: nlp-classifier-vm
   â”‚
   â–¼
Container: nlp-api (port 8000)
   â”‚
   â–¼
FastAPI + Models
```

### After UI Deployment
```
Internet
   â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚                 â”‚                 â”‚
   â–¼                 â–¼                 â–¼
Firewall       Firewall          Firewall
(port 22)      (port 8000)       (port 8501)
   â”‚                 â”‚                 â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
          VM: nlp-classifier-vm
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                         â”‚
        â–¼                         â–¼
   Container: nlp-api      Container: nlp-ui
   (port 8000)             (port 8501)
        â”‚                         â”‚
        â”‚                         â”‚
   FastAPI + Models          Streamlit
        â–²                         â”‚
        â”‚                         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           API calls (localhost)
```

---

## ğŸ“¦ File Structure Comparison

### Backend Files (EXISTING âœ…)
```
CLOUD-NLP-CLASSIFIER-GCP/
â”œâ”€â”€ Dockerfile                    âœ… Backend API
â”œâ”€â”€ src/api/server.py            âœ… FastAPI app
â”œâ”€â”€ models/                       âœ… ML models
â”‚   â”œâ”€â”€ baselines/
â”‚   â”œâ”€â”€ transformer/
â”‚   â””â”€â”€ toxicity_multi_head/
â””â”€â”€ scripts/
    â””â”€â”€ gcp-complete-deployment.ps1  âœ… Backend deploy script
```

### Frontend Files (TO CREATE)
```
CLOUD-NLP-CLASSIFIER-GCP/
â”œâ”€â”€ Dockerfile.streamlit          âœ… Exists (local mode)
â”œâ”€â”€ Dockerfile.streamlit.api      âŒ TO CREATE (API mode)
â”œâ”€â”€ src/ui/
â”‚   â”œâ”€â”€ streamlit_app.py         âœ… Exists (needs API mode)
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ api_inference.py     âŒ TO CREATE (API client)
â””â”€â”€ scripts/
    â””â”€â”€ gcp-deploy-ui.ps1        âŒ TO CREATE (UI deploy script)
```

---

## ğŸ”„ Deployment Workflow Comparison

### Backend Deployment Flow
```
Local Machine
    â”‚
    â”œâ”€ 1. Upload models to GCS (770 MB)
    â”‚
    â–¼
GCP Cloud Storage
    â”‚
    â–¼
GCP VM
    â”‚
    â”œâ”€ 2. Clone code from GitHub
    â”œâ”€ 3. Download models from GCS
    â”œâ”€ 4. Build Docker image (includes models)
    â”œâ”€ 5. Run container
    â”‚
    â–¼
Running API (port 8000)
```

### Frontend Deployment Flow (PLANNED)
```
Local Machine
    â”‚
    â”œâ”€ 1. Push code to GitHub (no models needed)
    â”‚
    â–¼
GitHub Repository
    â”‚
    â–¼
GCP VM (same as backend)
    â”‚
    â”œâ”€ 2. Pull latest code
    â”œâ”€ 3. Build Docker image (NO models, lightweight)
    â”œâ”€ 4. Run container with API_URL env var
    â”‚
    â–¼
Running UI (port 8501)
    â”‚
    â”œâ”€ Connects to API on localhost:8000
    â”‚
    â–¼
Full Stack Running
```

---

## ğŸ’° Cost Comparison

### Backend Only (CURRENT)
```
VM (e2-standard-2):     $49/month
Static IP:              $7/month
GCS Storage (1GB):      $0.02/month
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total:                  $56/month
```

### Backend + Frontend (AFTER)
```
VM (e2-standard-2):     $49/month  (no change)
Static IP:              $7/month   (no change)
GCS Storage (1GB):      $0.02/month (no change)
UI Container:           $0/month   (same VM)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total:                  $56/month  (NO INCREASE âœ…)
```

---

## âš¡ Performance Comparison

### Backend Container
- **Memory**: ~1.5GB (includes PyTorch + models)
- **CPU**: ~0.1-0.5 cores (idle/active)
- **Startup**: ~10 seconds (model loading)
- **Image Size**: ~2.5GB

### Frontend Container (EXPECTED)
- **Memory**: ~500MB (Streamlit + requests only)
- **CPU**: ~0.05-0.2 cores (idle/active)
- **Startup**: ~5 seconds (no model loading)
- **Image Size**: ~500MB

### Combined (Same VM)
- **Total Memory**: ~2GB / 8GB available (25% usage) âœ…
- **Total CPU**: ~0.15-0.7 cores / 2 available (35% usage) âœ…
- **Plenty of headroom** for traffic spikes âœ…

---

## ğŸ¯ Key Similarities (Why This Will Work)

Both deployments use:
1. âœ… Same VM infrastructure
2. âœ… Same GitHub repository
3. âœ… Same Docker pattern (build on VM)
4. âœ… Same error handling (`set -e`, `sudo docker`)
5. âœ… Same health check pattern
6. âœ… Same restart policy (`unless-stopped`)
7. âœ… Same validation approach (exit codes + output markers)
8. âœ… Same security (non-root user, firewall rules)

---

## ğŸ”‘ Key Differences (What Changes)

| Aspect | Backend | Frontend |
|--------|---------|----------|
| **Models** | Included in image | NOT included (calls API) |
| **Size** | Large (~2.5GB) | Small (~500MB) |
| **Build Time** | 5-10 minutes | 2-3 minutes |
| **Dependencies** | Heavy (PyTorch) | Light (Streamlit) |
| **Port** | 8000 | 8501 |
| **Health Endpoint** | `/health` | `/_stcore/health` |
| **Environment** | Minimal | Needs `API_URL` |

---

## ğŸ“ Implementation Checklist

### Code Changes
- [ ] Create `Dockerfile.streamlit.api` (API mode, no models)
- [ ] Create `src/ui/utils/api_inference.py` (API client)
- [ ] Modify `src/ui/streamlit_app.py` (support API mode)
- [ ] Test locally with docker-compose

### Infrastructure Changes
- [ ] Add firewall rule for port 8501
- [ ] Verify VM has capacity (it does âœ…)

### Deployment Script
- [ ] Create `scripts/gcp-deploy-ui.ps1`
- [ ] Follow backend script pattern exactly
- [ ] Add API connectivity verification
- [ ] Add comprehensive error handling

### Testing
- [ ] Local: docker-compose up (both services)
- [ ] Cloud: Deploy and test end-to-end
- [ ] Performance: Monitor resource usage
- [ ] Reliability: Test auto-restart

### Documentation
- [ ] Update README with UI URL
- [ ] Create UI deployment guide
- [ ] Add troubleshooting section
- [ ] Update architecture diagrams

---

## ğŸ‰ Expected Outcome

After successful deployment:

```
âœ… API Running:  http://35.232.76.140:8000
âœ… UI Running:   http://35.232.76.140:8501
âœ… Both containers on same VM
âœ… UI calls API via localhost (fast)
âœ… Auto-restart on failure
âœ… Health checks passing
âœ… No cost increase
âœ… Full-stack NLP app live!
```

---

## ğŸš€ Next Action

**READY TO IMPLEMENT** following this proven pattern:
1. Create API-mode files (Dockerfile, API client)
2. Create deployment script (following backend pattern)
3. Deploy to GCP
4. Test and verify
5. Document and celebrate! ğŸ‰

**Estimated Time**: 2 hours
**Risk**: LOW (following working pattern)
**Cost**: $0 additional
