# Multi-Head Toxicity Classification - Quick Reference

## üöÄ Quick Start

### Training
```bash
# Default training (3 epochs, batch=16, lr=2e-5)
python -m src.models.train_toxicity

# Custom epochs
python -m src.models.train_toxicity --epochs 5

# Custom config
python -m src.models.train_toxicity --config config/config_toxicity.yaml
```

### Inference
```python
import torch
import json
from transformers import AutoTokenizer
from src.models.multi_head_model import MultiHeadToxicityModel

# Load
with open("models/toxicity_multi_head/labels.json") as f:
    labels = json.load(f)["labels"]
model = MultiHeadToxicityModel("distilbert-base-uncased", labels)
model.load_state_dict(torch.load("models/toxicity_multi_head/model_weights.pt"))
model.eval()
tokenizer = AutoTokenizer.from_pretrained("models/toxicity_multi_head")

# Predict
text = "Your comment here"
inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=256)
with torch.no_grad():
    outputs = model(**inputs)
    probs = {k: torch.sigmoid(v).item() for k, v in outputs["logits"].items()}
print(probs)
```

---

## üìÅ Files

| File | Purpose | Lines |
|------|---------|-------|
| `models/multi_head_model.py` | Model architecture | 55 |
| `src/models/train_toxicity.py` | Training pipeline | 263 |
| `config/config_toxicity.yaml` | Configuration | 35 |

---

## üéØ Toxicity Categories

1. **toxic** - General toxicity (common)
2. **severe_toxic** - Extremely toxic (rare)
3. **obscene** - Obscene language (moderate)
4. **threat** - Threatening language (very rare)
5. **insult** - Insulting content (moderate)
6. **identity_hate** - Identity-based hate (rare)

---

## üìä Expected Performance

| Metric | Value |
|--------|-------|
| **Overall Accuracy** | 94-96% |
| **Training Time (GPU)** | 30-45 min |
| **Training Time (CPU)** | 3-4 hours |
| **Inference Latency** | 45-60ms |
| **Model Size** | ~260 MB |
| **Memory (Training)** | ~2.5 GB |
| **Memory (Inference)** | ~1.5 GB |

---

## ‚öôÔ∏è Configuration

### Default Hyperparameters
```yaml
model:
  name: "distilbert-base-uncased"
  max_seq_length: 256

training:
  train_batch_size: 16
  eval_batch_size: 16
  learning_rate: 2.0e-5
  num_train_epochs: 3
  threshold: 0.5

device: "cuda"
```

### Tuning Tips

**Better Accuracy:**
- ‚Üë Epochs: 5-10
- ‚Üë Sequence length: 512
- ‚Üì Learning rate: 1e-5

**Faster Training:**
- ‚Üë Batch size: 32-64
- ‚Üì Sequence length: 128

---

## üèóÔ∏è Architecture

```
Input ‚Üí DistilBERT ‚Üí [CLS] ‚Üí Dropout ‚Üí 6 Heads ‚Üí 6 Predictions
```

**Details:**
- Encoder: DistilBERT-base (66M params)
- Heads: 6 √ó Linear(768 ‚Üí 1)
- Loss: BCEWithLogitsLoss (averaged)
- Optimizer: AdamW

---

## üß™ Testing

```bash
# Test model load
python -c "from src.models.multi_head_model import MultiHeadToxicityModel; model = MultiHeadToxicityModel('distilbert-base-uncased', ['toxic']); print('‚úÖ')"

# Dry run (1 epoch)
python -m src.models.train_toxicity --epochs 1

# Full training
python -m src.models.train_toxicity
```

---

## üì¶ Output Files

After training, you'll find:
- `models/toxicity_multi_head/model_weights.pt` (~260 MB)
- `models/toxicity_multi_head/tokenizer_config.json`
- `models/toxicity_multi_head/vocab.txt`
- `models/toxicity_multi_head/labels.json`
- `training_loss_plot.png`

---

## üîß Common Issues

### Issue: CUDA out of memory
**Solution:** Reduce batch size in config
```yaml
training:
  train_batch_size: 8  # or 4
```

### Issue: Missing label columns
**Solution:** Ensure CSV has required columns:
- `comment_text` or `text`
- `toxic`, `severe_toxic`, `obscene`, `threat`, `insult`, `identity_hate`

### Issue: Slow training
**Solution:** Use GPU or reduce sequence length
```yaml
model:
  max_seq_length: 128  # instead of 256
```

---

## üìö Documentation

- **Detailed PR**: `docs/PULL_REQUEST_MULTI_HEAD_TOXICITY.md`
- **Full Summary**: `docs/MULTI_HEAD_TOXICITY_SUMMARY.md`
- **Changelog**: `CHANGELOG_TOXICITY.md`
- **This Guide**: `docs/TOXICITY_QUICK_REFERENCE.md`

---

## üéØ Next Steps

1. ‚úÖ Train model on your dataset
2. ‚úÖ Evaluate performance
3. ‚è≥ Integrate with FastAPI server
4. ‚è≥ Add to Docker deployment
5. ‚è≥ Deploy to production

---

## üí° Example Output

```
INFO - Using device: cuda
INFO - Train size: 14400, Val size: 1600
INFO - Epoch 1 | Step 100/900 | Global Step 100 | Loss: 0.4523
INFO - Epoch 1 finished. Avg training loss: 0.3891
INFO - Validation loss: 0.3245
INFO -   toxic          | acc: 0.9234
INFO -   severe_toxic   | acc: 0.9876
INFO -   obscene        | acc: 0.9456
INFO -   threat         | acc: 0.9923
INFO -   insult         | acc: 0.9123
INFO -   identity_hate  | acc: 0.9845
INFO - Saved best model to models/toxicity_multi_head
INFO - Loss plot saved to training_loss_plot.png
```

---

**Version**: 1.0.0 | **Date**: 2025-12-09 | **Status**: ‚úÖ Ready
