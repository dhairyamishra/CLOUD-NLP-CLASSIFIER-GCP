# Configuration for Transformer Model (DistilBERT)
# FULL-SCALE LOCAL TRAINING - Comprehensive configuration with early stopping

# Model settings
model:
  name: "distilbert-base-uncased"
  num_labels: null  # Will be set automatically based on dataset
  max_seq_length: 256  # FULL SCALE: Longer sequences for better context understanding
  dropout: 0.1  # Dropout rate for regularization
  attention_dropout: 0.1  # Attention dropout rate

# Training hyperparameters (FULL-SCALE LOCAL MODE)
training:
  train_batch_size: 32  # Balanced batch size for local GPU/CPU
  eval_batch_size: 64   # Larger batch for evaluation
  learning_rate: 3.0e-5  # FULL SCALE: Slightly higher for better convergence
  num_train_epochs: 15   # FULL SCALE: More epochs with early stopping
  weight_decay: 0.01     # L2 regularization
  warmup_ratio: 0.15     # FULL SCALE: 15% warmup steps for stable training
  warmup_steps: 0        # Alternative to warmup_ratio (set to 0 to use ratio)
  gradient_accumulation_steps: 2  # FULL SCALE: Effective batch = 32 * 2 = 64
  max_grad_norm: 1.0     # Gradient clipping
  
  # Optimization
  fp16: false  # Set to true if GPU supports mixed precision (NVIDIA with Tensor Cores)
  fp16_opt_level: "O1"  # Options: O0, O1, O2, O3 (O1 recommended)
  optim: "adamw_torch"  # Optimizer: adamw_torch, adamw_hf, adafactor
  adam_beta1: 0.9       # Adam beta1 parameter
  adam_beta2: 0.999     # Adam beta2 parameter
  adam_epsilon: 1.0e-8  # Adam epsilon parameter
  
  # DataLoader settings
  dataloader_num_workers: 0  # WINDOWS FIX: Set to 0 to avoid multiprocessing hangs (use 2-4 on Linux/Mac)
  dataloader_pin_memory: true  # Speeds up CPU-to-GPU transfer
  dataloader_drop_last: false  # Keep all samples
  
  # Early stopping - COMPREHENSIVE
  early_stopping:
    enabled: true  # CRITICAL: Enable early stopping
    patience: 5    # FULL SCALE: Stop if no improvement for 5 evaluation cycles
    metric: "eval_f1_macro"  # Primary metric to monitor
    mode: "max"    # "max" for metrics to maximize, "min" for loss
    min_delta: 0.001  # Minimum change to qualify as improvement
    restore_best_weights: true  # Restore best model weights after training
  
  # Learning rate scheduler - ADVANCED
  # Options: linear, cosine, cosine_with_restarts, polynomial, constant, constant_with_warmup
  lr_scheduler:
    type: "cosine_with_restarts"  # FULL SCALE: Cosine with restarts for better convergence
    num_cycles: 3  # Number of restart cycles (for cosine_with_restarts)
  
  # Logging and checkpointing - DETAILED
  logging_steps: 25      # FULL SCALE: More frequent logging
  eval_steps: 100        # FULL SCALE: Evaluate every 100 steps
  save_steps: 100        # FULL SCALE: Save checkpoint every 100 steps
  save_total_limit: 5    # FULL SCALE: Keep 5 best checkpoints
  load_best_model_at_end: true  # Load best model at end of training
  metric_for_best_model: "f1_macro"  # Metric to determine best model
  greater_is_better: true  # True for accuracy/F1, False for loss

# Data paths
data:
  train_path: "data/processed/train.csv"
  val_path: "data/processed/val.csv"
  test_path: "data/processed/test.csv"

# Model save paths
model_save_dir: "models/transformer/distilbert"

# Reproducibility
seed: 42

# Evaluation settings
evaluation:
  metrics:
    - "accuracy"
    - "f1_macro"
    - "f1_weighted"
    - "precision_macro"
    - "recall_macro"
  save_confusion_matrix: true
  save_classification_report: true
  compute_roc_auc: true  # For binary classification or OvR

# Device settings
device: "cuda"  # Options: "cuda", "cpu", "mps" (for Mac M1/M2)
